{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analyzing Patents With USPTO Open Data\n",
    "\n",
    "Welcome! This Jupyter Notebook will go through the process of reading patent data, analyzing the word usage in the patent abstracts, and identifying words which are used more frequently by city or state. Hopefully, we can learn about regional differences in patent filing.\n",
    "\n",
    "This analysis is possible because the USPTO makes bulk patent data available online through a collaboration with Reed Tech (http://patents.reedtech.com/). \n",
    "\n",
    "Questions, comments, suggestions, and corrections can be directed to mgebhard@gmail.com.\n",
    "\n",
    "If you want to work through the code and follow along on your own computer, you'll need the python packages: lxml, pandas, sklearn, re, and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lxml.etree as ET\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Preprocess the Data\n",
    "\n",
    "1. Download .ZIP files from http://patents.reedtech.com/pgrbft.php. Because of the different formatting, we'll stick to files that fall into 2001-Present. Each file contains all of the patents issued by the USPTO for one week. These files are each around 100 MB zipped and unzip to 300-700 MB so we can't work with too many at once. Extract them to the working directory.\n",
    "2. The more patent data we can analyze at once, the more our word frequency output will be due to true regional differences and not just low sample sizes. Let's make a list of file paths from some recent releases so we can process multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = ['ipg160119.xml', 'ipg150120.xml', 'ipg160816.xml', 'ipg160809.xml', 'ipg160802.xml',\n",
    "         'ipg160719.xml', 'ipg160726.xml']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to extract location data and the abstract text from each patent in these files. Unfortunately, the files are not properly constructed .XML files but instead contain a concatenation of individual .XML files for each patent, which makes reading them more difficult. We have to use a bit of code to read these files and extract the data to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class patent(object):\n",
    "    def __init__(self, location, abstract):\n",
    "        self.location = location\n",
    "        self.abstract = abstract\n",
    "    def getLocation(self):\n",
    "        return self.location\n",
    "    def getAbstract(self):\n",
    "        return self.abstract\n",
    "    \n",
    "def filesToDataFrame(files):\n",
    "    patents = []\n",
    "    for path in files:\n",
    "        with open(path, 'rb') as f:\n",
    "            lines = f.readlines()\n",
    "        patent_entries_start = []\n",
    "        for line in range(len(lines)):\n",
    "            if lines[line][0:5] == '<?xml':\n",
    "                patent_entries_start.append(line)\n",
    "        for i in range(len(patent_entries_start)-1):\n",
    "            start = patent_entries_start[i]\n",
    "            stop = patent_entries_start[i+1]\n",
    "            patent_entry = lines[start:stop]\n",
    "            location, abstract = process_entry(patent_entry)\n",
    "            patents.append(patent(location, abstract))\n",
    "    patent_df = pd.DataFrame.from_records([(x.getLocation()[0], \n",
    "                                            x.getLocation()[1], \n",
    "                                            x.getLocation()[2], \n",
    "                                            x.getAbstract()) for x in patents], \n",
    "                                              columns=['city', 'state', 'country', 'abstract'])\n",
    "    patent_df.dropna(inplace=True)\n",
    "    return patent_df\n",
    "\n",
    "def process_entry(entry):\n",
    "    with open('temp.xml', 'w') as xml_entry:\n",
    "        for line in entry:\n",
    "            xml_entry.write(line)\n",
    "    context = ET.iterparse('temp.xml', events=('start', 'end'))\n",
    "    city = None\n",
    "    state = None\n",
    "    country = None\n",
    "    paragraph = None\n",
    "    for event, elem in iter(context):\n",
    "        if event == 'start' and elem.tag == 'inventor':\n",
    "            for child in elem:\n",
    "                for grandchild in child:\n",
    "                    for info in grandchild:\n",
    "                        if info.tag == 'city':\n",
    "                            city = info.text\n",
    "                        if info.tag == 'state':\n",
    "                            state = info.text\n",
    "                        if info.tag == 'country':\n",
    "                            country = info.text\n",
    "        if event == 'start' and elem.tag == 'abstract':\n",
    "            paragraph = elem.findtext('p')\n",
    "    return (city, state, country), paragraph\n",
    "\n",
    "patent_df = filesToDataFrame(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the dataframe we've constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17333 entries, 252 to 38750\n",
      "Data columns (total 4 columns):\n",
      "city        17333 non-null object\n",
      "state       17333 non-null object\n",
      "country     17333 non-null object\n",
      "abstract    17333 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 677.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13769</th>\n",
       "      <td>Hillsborough</td>\n",
       "      <td>CA</td>\n",
       "      <td>US</td>\n",
       "      <td>The present document relates to methods and sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8898</th>\n",
       "      <td>Kirkland</td>\n",
       "      <td>WA</td>\n",
       "      <td>US</td>\n",
       "      <td>Supplemental computing devices that provide pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34189</th>\n",
       "      <td>Morganville</td>\n",
       "      <td>NJ</td>\n",
       "      <td>US</td>\n",
       "      <td>A mobile device operates in a standard mode of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36431</th>\n",
       "      <td>Phelan</td>\n",
       "      <td>CA</td>\n",
       "      <td>US</td>\n",
       "      <td>The ceiling fan with air ionizing fan blades i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33275</th>\n",
       "      <td>Yorktown Heights</td>\n",
       "      <td>NY</td>\n",
       "      <td>US</td>\n",
       "      <td>An approach to forming a semiconductor structu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31888</th>\n",
       "      <td>Gilbert</td>\n",
       "      <td>AZ</td>\n",
       "      <td>US</td>\n",
       "      <td>A process produces a time release pesticide gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34008</th>\n",
       "      <td>Lawrenceville</td>\n",
       "      <td>GA</td>\n",
       "      <td>US</td>\n",
       "      <td>A system for maintaining an address book, wher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24503</th>\n",
       "      <td>Berlin</td>\n",
       "      <td>CT</td>\n",
       "      <td>US</td>\n",
       "      <td>A rifle configured for firing a 7.62Ã—39 mm rou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32057</th>\n",
       "      <td>Staatsburg</td>\n",
       "      <td>NY</td>\n",
       "      <td>US</td>\n",
       "      <td>Execution of instructions in a transactional e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14336</th>\n",
       "      <td>San Diego</td>\n",
       "      <td>CA</td>\n",
       "      <td>US</td>\n",
       "      <td>A system for producing an exclusionary buffer ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   city state country  \\\n",
       "13769      Hillsborough    CA      US   \n",
       "8898           Kirkland    WA      US   \n",
       "34189       Morganville    NJ      US   \n",
       "36431            Phelan    CA      US   \n",
       "33275  Yorktown Heights    NY      US   \n",
       "31888           Gilbert    AZ      US   \n",
       "34008     Lawrenceville    GA      US   \n",
       "24503            Berlin    CT      US   \n",
       "32057        Staatsburg    NY      US   \n",
       "14336         San Diego    CA      US   \n",
       "\n",
       "                                                abstract  \n",
       "13769  The present document relates to methods and sy...  \n",
       "8898   Supplemental computing devices that provide pr...  \n",
       "34189  A mobile device operates in a standard mode of...  \n",
       "36431  The ceiling fan with air ionizing fan blades i...  \n",
       "33275  An approach to forming a semiconductor structu...  \n",
       "31888  A process produces a time release pesticide gr...  \n",
       "34008  A system for maintaining an address book, wher...  \n",
       "24503  A rifle configured for firing a 7.62Ã—39 mm rou...  \n",
       "32057  Execution of instructions in a transactional e...  \n",
       "14336  A system for producing an exclusionary buffer ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patent_df.info()\n",
    "patent_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Word Frequencies in a String of Text\n",
    "\n",
    "We now want to write a function that can take in our dataframe (or a piece of it) and keep track of word usage in the abstracts that appear in the dataframe. To do this, we take the following steps:\n",
    "1. Combine all of the abstracts into one long string.\n",
    "2. Process the string to remove uppercase letters and non-alphanumeric characters.\n",
    "3. Use CountVectorizer from sklearn.feature_extraction.text to convert the text to a matrix of token counts. This is especially useful because it removes \"stop words\" which are short, common words that don't have meaning for our purpose. We can also limit the vocab size using max_features. In other words, we can choose to only look at, for example, the top 200 most frequent words.\n",
    "    Documentation here: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOverallWordFrequency(patent_df, vocab_size):\n",
    "    combinedWords = ''\n",
    "    for entry in patent_df['abstract']:\n",
    "        combinedWords += entry + ' '\n",
    "    text = processText(combinedWords)\n",
    "    cv = CountVectorizer(min_df=0, \n",
    "                         decode_error='ignore', \n",
    "                         stop_words='english', \n",
    "                         max_features=vocab_size)\n",
    "    counts = cv.fit_transform([text]).toarray().ravel()\n",
    "    words = np.array(cv.get_feature_names())\n",
    "    counts = counts / float(counts.max())\n",
    "    return zip(words, counts)\n",
    "\n",
    "def processText(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = text.split()\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Regional Word Frequencies to Overall Word Frequencies\n",
    "\n",
    "Now that we can generate a list of words and their occurrances in the patent abstracts, we need to be able to compare a list created from a region with the list created from the overall data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getHighestScoredWordInRegion(regionalUsage, overallUsage):\n",
    "    highestScore = 0\n",
    "    wordWithHighestScore = None\n",
    "    for wordAndFreq in regionalUsage:\n",
    "        overallFreq = getOverallFreq(wordAndFreq[0], overallUsage)\n",
    "        score = getScore(wordAndFreq[1], overallFreq)\n",
    "        if score > highestScore:\n",
    "            wordWithHighestScore = wordAndFreq[0]\n",
    "            highestScore = score\n",
    "    return wordWithHighestScore\n",
    "\n",
    "def getOverallFreq(word, overallUsage):\n",
    "    for wordAndFreq in overallUsage:\n",
    "        if word == wordAndFreq[0]:\n",
    "            return wordAndFreq[1]\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we used the getScore function to assign a score to a word based on its frequency in the region compared to its frequency in the overall data. A simple scoring function is just the ratio of the occurrance in the region to the occurrance in the overall data. Other scoring functions will, of course, identify different patterns in word usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getScore(singleFreq, overallFreq):\n",
    "    return (singleFreq / overallFreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put this all together into a final function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bestWordInRegion(region, city=False, state=False):\n",
    "    overallUsage = getOverallWordFrequency(patent_df, 100000)\n",
    "    output = []\n",
    "    for location in region:\n",
    "        if city:\n",
    "            regionalUsage = getOverallWordFrequency(patent_df[patent_df['city'] == location], 500)\n",
    "            output.append((location, \n",
    "                           len(patent_df[patent_df['city'] == location].index), \n",
    "                           getHighestScoredWordInRegion(regionalUsage, overallUsage)))\n",
    "        if state:\n",
    "            regionalUsage = getOverallWordFrequency(patent_df[patent_df['state'] == location], 200)\n",
    "            output.append((location, \n",
    "                           len(patent_df[patent_df['state'] == location].index), \n",
    "                           getHighestScoredWordInRegion(regionalUsage, overallUsage))) \n",
    "    output_df = pd.DataFrame.from_records(output, columns=['location', 'patents analyzed', 'frequent word'])\n",
    "    print output_df\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at Word Usage by State and City\n",
    "\n",
    "The function bestWordInRegion takes in a list of city or state regions and tells us the word for each region that stand out. Let's look at states. Alaska is left off since it has so few patents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   location  patents analyzed  frequent word\n",
      "0        AL                74            ejb\n",
      "1        AR                29          cl172\n",
      "2        CA              4969          clock\n",
      "3        CO               347     television\n",
      "4        CT               264       buttress\n",
      "5        DE                37            tfe\n",
      "6        FL               525        antenna\n",
      "7        GA               291         carton\n",
      "8        HI                12       forearms\n",
      "9        ID                92   microfeature\n",
      "10       IL               579      hydraulic\n",
      "11       IN               229        humeral\n",
      "12       IA               104    ethanologen\n",
      "13       KS                94  concatenation\n",
      "14       KY                99          rolls\n",
      "15       LA                54   hydrocyclone\n",
      "16       ME                34       hookworm\n",
      "17       MD               249     mesothelin\n",
      "18       MI               678     telematics\n",
      "19       MN               511         pacing\n",
      "20       MS                16         asador\n",
      "21       MO               144       20453014\n",
      "22       MT                22    antisolvent\n",
      "23       NE                50           muc4\n",
      "24       NV                86         patron\n",
      "25       NH               111       monocore\n",
      "26       NJ               558     subscriber\n",
      "27       NM                35           lccd\n",
      "28       NY              1037            fin\n",
      "29       NC               453        droplet\n",
      "30       ND                 7        loaders\n",
      "31       OH               381           tire\n",
      "32       OK                64      acidizing\n",
      "33       OR               383        feather\n",
      "34       PA               446       fixation\n",
      "35       RI                49      calibrant\n",
      "36       SC               118            phr\n",
      "37       SD                16       barreled\n",
      "38       TN               138     rotisserie\n",
      "39       TX              1211       wellbore\n",
      "40       UT               173            pcd\n",
      "41       VT                47            rdf\n",
      "42       VA               249        webbing\n",
      "43       WA               812       aircraft\n",
      "44       WV                12       succinic\n",
      "45       WI               246           bean\n",
      "46       WY                 7       scatting\n"
     ]
    }
   ],
   "source": [
    "region_states = ['AL', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', \n",
    "                 'KY', 'LA', 'ME', 'MD', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY',\n",
    "                 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA',\n",
    "                 'WV', 'WI', 'WY']\n",
    "bestWordInRegion(region_states, state=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can look at patents by city. Granted, this may not be as accurate as analyzing by state for a few reasons. (1) There are likely fewer patents from a city than from a state. (2) The location is based on only one inventor when there are often multiple inventors listed per patent. (3) The location is based on the inventor's residence address and not necessarily the city where the work was done.\n",
    "\n",
    "But analyzing by city can still be interesting. Let's look at some cities in the Bay Area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               location  patents analyzed     frequent word\n",
      "0              San Jose               507               dbi\n",
      "1           Santa Clara               138  chemonucleolysis\n",
      "2             Sunnyvale               242             chord\n",
      "3             Cupertino               162             asisp\n",
      "4         Mountain View               187      whitelisting\n",
      "5             Palo Alto               163          antifuse\n",
      "6             Los Altos                74    disambiguation\n",
      "7            Menlo Park                58        guidepiece\n",
      "8          Redwood City                49          airspace\n",
      "9             San Mateo                47           bureaus\n",
      "10  South San Francisco                 7               c10\n",
      "11        San Francisco               325            mentor\n",
      "12             Berkeley                34      choreography\n",
      "13              Oakland                42             actor\n"
     ]
    }
   ],
   "source": [
    "region_cities = ['San Jose', 'Santa Clara', 'Sunnyvale', 'Cupertino', 'Mountain View', 'Palo Alto',\n",
    "                 'Los Altos', 'Menlo Park', 'Redwood City', 'San Mateo', 'South San Francisco',\n",
    "                 'San Francisco', 'Berkeley', 'Oakland']\n",
    "bestWordInRegion(region_cities, city=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "What did I learn technically?\n",
    "\n",
    "* Non-standard .XML files can be difficult to work with, but lxml.etree can still be useful in extracting the desired data.\n",
    "\n",
    "* CountVectorizer from sklearn.feature_extraction.text is incredibly helpful for analyzing large amounts of text.\n",
    "\n",
    "What did I learn from the patents?\n",
    "\n",
    "* Certain state/word pairings make intuitive sense and confirm that our word search algorithm is probably working correctly: Texas/wellbore, Iowa/ethanologen, Michigan/telematics.\n",
    "\n",
    "* Some words that show up seem random but convey information. Missouri's 20453014 is a soybean variant.\n",
    "\n",
    "# Future\n",
    "\n",
    "* Add more patent data to improve results.\n",
    "\n",
    "* Adjust the vocab_size and scoring function to find more representative words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
